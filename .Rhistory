library(tableone)
library(plyr)
library(dplyr)
library(rlang)
library(tidyr)
library(car)
library(corrr)
library(emmeans)
library(doParallel)
library(foreach)
library(yaml)
library(magrittr)
library(digest)
library(reticulate)
library(caret)
library(sparklyr)
library(yardstick)
library(forcats)
library(mice)
library(DMwR)
library(keras)
library(reticulate)
library(tensorflow)
library(PerfMeas)
packageVersion("keras")
packageVersion("tensorflow")
# 1a. Loading the dataframe
df <- read.xlsx("D://Google Drive/Research/Bergman Clinics/ML Deep Learning/ML Predicting outcomes degenerative spine/DATA.old.xlsx", sheet = 2)
str(df)
# 1b. Change data types to factors and numbers where applicable
df$supervised <- factor(df$supervised)
df$prior <- factor(df$prior)
df$female <- factor(df$female)
df$smoking <- factor(df$smoking)
df$alcohol1ja <- factor(df$alcohol1ja)
df$drugs <- factor(df$drugs)
df$farlateral <- factor(df$farlateral)
df$stenosis <- factor(df$stenosis)
df$sequester <- factor(df$sequester)
df$listhesis <- factor(df$listhesis)
df$bulgingonly <- factor(df$bulgingonly)
df$ASAScore <- factor(df$ASAScore)
df$level <- factor(df$level)
df$side <- factor(df$side)
df$mcidodi <- factor(df$mcidodi, levels = c(1,0))
df$mcidback <- factor(df$mcidback, levels = c(1,0))
df$mcidleg <- factor(df$mcidleg, levels = c(1,0))
str(df)
# save a dataset with only labelled data
dfl <- df[df$supervised==1,]
str(dfl)
tail(dfl)
head(dfl)
# 2a. Multiple Imputation using mice
set.seed(123)
md <- md.pattern(dfl) #quantify missing data
md
tempData <- mice(dfl, m = 5, maxit = 20, meth = 'pmm', seed = 123)
summary(tempData)
dfl <- complete(tempData,1)
# 2b. Adjust class imbalance using SMOTE
table(dfl$mcidleg)
prop.table(table(dfl$mcidleg)) # quantify class imbalance
dfl <- SMOTE(mcidleg ~ ., dfl, perc.over = 100, perc.under = 300, k = 5)
table(dfl$mcidleg)
prop.table(table(dfl$mcidleg)) # quantify class imbalance again
#normalize data
dflnorm <- scale(dfl[c(12,14,15,16,19,20,21)])
dfl <- as.data.frame(cbind(dflnorm, dfl[c(1:11,13,17,18,22,23,24)]))
# 2c. Save copy of df as "df2" for analysis with conventional statistical methods later on
dfl2 <- dfl
summary(dfl)
##### Train Keras Model
# 3a. Change dataframe to matrix for Keras
dfl <- as.matrix(dfl)
dimnames(dfl) <- NULL
# 3b. Normalization
#df[, 1:16] <- normalize(df[, 1:16])
#summary(df)
# 3c. SPlit dataset for testing final
set.seed(123)
dt = sort(sample(nrow(dfl), nrow(dfl)*.8))
train <-dfl[dt,]
test <-dfl[-dt,]
### TRAIN your model
# Use all of your CPU cores
#cl <- makePSOCKcluster(30)
#registerDoParallel(cl)
trainset <- train[, 1:21]
testset <- test[, 1:21]
traintarget <- train[, 24]
testtarget <- test[, 24]
# 4a. Build you model in Keras / TensorFlow - One hot encoding
#Onehotencoding
library(dplyr)
trainLabels <- to_categorical(traintarget)
testLabels <- to_categorical(testtarget)
print(testLabels)
print(trainLabels)
library(keras)
library(caret)
library(tfruns)
FLAGS <- tfruns::flags(
flag_numeric('dropout1', 0.8, 'drop1'),
flag_numeric('dropout2', 0.5, "drop2"),
flag_numeric("dropout3", 0, "drop3"),
flag_numeric("dropout4", 0, "drop4"),
flag_integer('u1', 256, 'Units1'),
flag_integer("u2", 128, "units2"),
flag_integer("u3", 64, "units3"),
flag_integer("u4", 32, "units4"),
flag_integer("u5", 4, "units5"),
flag_integer("batch", 5, "batchsize"),
flag_integer("epoch", 200, "epochs"),
flag_numeric("alpha", 0.002, "learningrate")
)
#Create Sequential Model
model_keras <- keras_model_sequential()
model_keras %>%
layer_dense(units = FLAGS$u1, activation = "relu", input_shape = c(21)) %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout1) %>%
layer_dense(units = FLAGS$u2, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout2) %>%
layer_dense(units = FLAGS$u3, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout3) %>%
layer_dense(units = FLAGS$u4, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout4) %>%
layer_dense(units = FLAGS$u5, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dense(units = 2, activation = "softmax")
summary(model_keras)
#Compile (Watch out for binary/categorical corssentropy!!!!)
model_keras %>%
compile(loss = "binary_crossentropy",
optimizer = optimizer_adam(lr = FLAGS$alpha, decay = 1e-8),
metrics = "accuracy")
#Train Model
history <- model_keras %>%
fit(trainset,
trainLabels,
epoch = FLAGS$epoch,
batch_size = FLAGS$batch,
validation_split = 0.2,
verbose = 1,
class_weight = list("0" = 2, "1" = 1))
plot(history)
score <- model_keras %>% evaluate(
testset, testLabels,
verbose = 1
)
cat('Test loss:', score[[1]], '\n')
cat('Test accuracy:', score[[2]], '\n')
#Evaluate Model with Test data
#Evaluate Model with Test data
library(ROCR)
library(caret)
accuracy <- model_keras %>%
evaluate(testset, testLabels)
accuracy
prob <- model_keras %>%
predict_proba(testset)
pred <- model_keras %>%
predict_classes(testset) %>% as.vector()
table(Predicted = pred, Actual = testtarget)
rocdata <- data.frame(cbind(prob, pred, testtarget))
str(rocdata)
rocdata$probability <- as.numeric(as.vector(rocdata$V2))
rocdata$prediction <- as.numeric(as.vector(rocdata$pred))
rocdata$target <- as.numeric(as.vector(rocdata$testtarget))
predroc <- prediction(rocdata$probability, rocdata$target)
auc <- performance(predroc, "auc")
auc <- unlist(slot(auc, "y.values"))
auc <- round(auc,3)
roc1 <- performance(predroc, "tpr", "fpr")
plot(roc1)
auc
confmatrix <- confusionMatrix(as.factor(rocdata$prediction), as.factor(rocdata$target), positive = "1")
accuracy <- confmatrix[["overall"]][["Accuracy"]]
sensitivity <- confmatrix[["byClass"]][["Sensitivity"]]
specificity <- confmatrix[["byClass"]][["Specificity"]]
F1 <- confmatrix[["byClass"]][["F1"]]
detectionrate <- confmatrix[["byClass"]][["Detection Rate"]]
precision <- confmatrix[["byClass"]][["Pos Pred Value"]]
#Get data out of loop
data.frame(cbind(auc, accuracy, sensitivity, specificity, precision, F1, detectionrate))
# Setup: Load the required packages into your computer's memory. If you have not yet installed the packages, install them
# using "install.packages("packagename")
library(ggplot2)
library(ggthemes)
library(plotROC)
library(ROCR)
library(pROC)
library(knitr)
library(openxlsx)
library(readxl)
library(epiR)
library(reshape2)
library(ggthemes)
library(tableone)
library(plyr)
library(dplyr)
library(rlang)
library(tidyr)
library(car)
library(corrr)
library(emmeans)
library(doParallel)
library(foreach)
library(yaml)
library(magrittr)
library(digest)
library(reticulate)
library(caret)
library(sparklyr)
library(yardstick)
library(forcats)
library(mice)
library(DMwR)
library(keras)
library(reticulate)
library(tensorflow)
library(PerfMeas)
packageVersion("keras")
packageVersion("tensorflow")
# 1a. Loading the dataframe
df <- read.xlsx("D://Google Drive/Research/Bergman Clinics/ML Deep Learning/ML Predicting outcomes degenerative spine/DATA.old.xlsx", sheet = 2)
str(df)
# 1b. Change data types to factors and numbers where applicable
df$supervised <- factor(df$supervised)
df$prior <- factor(df$prior)
df$female <- factor(df$female)
df$smoking <- factor(df$smoking)
df$alcohol1ja <- factor(df$alcohol1ja)
df$drugs <- factor(df$drugs)
df$farlateral <- factor(df$farlateral)
df$stenosis <- factor(df$stenosis)
df$sequester <- factor(df$sequester)
df$listhesis <- factor(df$listhesis)
df$bulgingonly <- factor(df$bulgingonly)
df$ASAScore <- factor(df$ASAScore)
df$level <- factor(df$level)
df$side <- factor(df$side)
df$mcidodi <- factor(df$mcidodi, levels = c(1,0))
df$mcidback <- factor(df$mcidback, levels = c(1,0))
df$mcidleg <- factor(df$mcidleg, levels = c(1,0))
str(df)
# save a dataset with only labelled data
dfl <- df[df$supervised==1,]
str(dfl)
tail(dfl)
head(dfl)
# 2a. Multiple Imputation using mice
set.seed(123)
md <- md.pattern(dfl) #quantify missing data
md
tempData <- mice(dfl, m = 5, maxit = 20, meth = 'pmm', seed = 123)
summary(tempData)
dfl <- complete(tempData,1)
# 2b. Adjust class imbalance using SMOTE
table(dfl$mcidleg)
prop.table(table(dfl$mcidleg)) # quantify class imbalance
dfl <- SMOTE(mcidleg ~ ., dfl, perc.over = 100, perc.under = 300, k = 5)
table(dfl$mcidleg)
prop.table(table(dfl$mcidleg)) # quantify class imbalance again
#normalize data
dflnorm <- scale(dfl[c(12,14,15,16,19,20,21)])
dfl <- as.data.frame(cbind(dflnorm, dfl[c(1:11,13,17,18,22,23,24)]))
# 2c. Save copy of df as "df2" for analysis with conventional statistical methods later on
dfl2 <- dfl
summary(dfl)
##### Train Keras Model
# 3a. Change dataframe to matrix for Keras
dfl <- as.matrix(dfl)
dimnames(dfl) <- NULL
# 3b. Normalization
#df[, 1:16] <- normalize(df[, 1:16])
#summary(df)
# 3c. SPlit dataset for testing final
set.seed(123)
dt = sort(sample(nrow(dfl), nrow(dfl)*.8))
train <-dfl[dt,]
test <-dfl[-dt,]
### TRAIN your model
# Use all of your CPU cores
#cl <- makePSOCKcluster(30)
#registerDoParallel(cl)
trainset <- train[, 1:21]
testset <- test[, 1:21]
traintarget <- train[, 24]
testtarget <- test[, 24]
# 4a. Build you model in Keras / TensorFlow - One hot encoding
#Onehotencoding
library(dplyr)
trainLabels <- to_categorical(traintarget)
testLabels <- to_categorical(testtarget)
print(testLabels)
print(trainLabels)
library(keras)
library(caret)
library(tfruns)
FLAGS <- tfruns::flags(
flag_numeric('dropout1', 0.8, 'drop1'),
flag_numeric('dropout2', 0.5, "drop2"),
flag_numeric("dropout3", 0, "drop3"),
flag_numeric("dropout4", 0, "drop4"),
flag_integer('u1', 256, 'Units1'),
flag_integer("u2", 128, "units2"),
flag_integer("u3", 64, "units3"),
flag_integer("u4", 32, "units4"),
flag_integer("u5", 4, "units5"),
flag_integer("batch", 5, "batchsize"),
flag_integer("epoch", 200, "epochs"),
flag_numeric("alpha", 0.002, "learningrate")
)
#Create Sequential Model
model_keras <- keras_model_sequential()
model_keras %>%
layer_dense(units = FLAGS$u1, activation = "relu", input_shape = c(21)) %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout1) %>%
layer_dense(units = FLAGS$u2, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout2) %>%
layer_dense(units = FLAGS$u3, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout3) %>%
layer_dense(units = FLAGS$u4, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dropout(rate = FLAGS$dropout4) %>%
layer_dense(units = FLAGS$u5, activation = "relu") %>%
#layer_batch_normalization() %>%
layer_dense(units = 2, activation = "softmax")
summary(model_keras)
#Compile (Watch out for binary/categorical corssentropy!!!!)
model_keras %>%
compile(loss = "binary_crossentropy",
optimizer = optimizer_adam(lr = FLAGS$alpha, decay = 1e-8),
metrics = "accuracy")
#Train Model
history <- model_keras %>%
fit(trainset,
trainLabels,
epoch = FLAGS$epoch,
batch_size = FLAGS$batch,
validation_split = 0.2,
verbose = 1,
class_weight = list("0" = 2, "1" = 1))
plot(history)
score <- model_keras %>% evaluate(
testset, testLabels,
verbose = 1
)
cat('Test loss:', score[[1]], '\n')
cat('Test accuracy:', score[[2]], '\n')
#Evaluate Model with Test data
#Evaluate Model with Test data
library(ROCR)
library(caret)
accuracy <- model_keras %>%
evaluate(testset, testLabels)
accuracy
prob <- model_keras %>%
predict_proba(testset)
pred <- model_keras %>%
predict_classes(testset) %>% as.vector()
table(Predicted = pred, Actual = testtarget)
rocdata <- data.frame(cbind(prob, pred, testtarget))
str(rocdata)
rocdata$probability <- as.numeric(as.vector(rocdata$V2))
rocdata$prediction <- as.numeric(as.vector(rocdata$pred))
rocdata$target <- as.numeric(as.vector(rocdata$testtarget))
predroc <- prediction(rocdata$probability, rocdata$target)
auc <- performance(predroc, "auc")
auc <- unlist(slot(auc, "y.values"))
auc <- round(auc,3)
roc1 <- performance(predroc, "tpr", "fpr")
plot(roc1)
auc
confmatrix <- confusionMatrix(as.factor(rocdata$prediction), as.factor(rocdata$target), positive = "1")
accuracy <- confmatrix[["overall"]][["Accuracy"]]
sensitivity <- confmatrix[["byClass"]][["Sensitivity"]]
specificity <- confmatrix[["byClass"]][["Specificity"]]
F1 <- confmatrix[["byClass"]][["F1"]]
detectionrate <- confmatrix[["byClass"]][["Detection Rate"]]
precision <- confmatrix[["byClass"]][["Pos Pred Value"]]
#Get data out of loop
data.frame(cbind(auc, accuracy, sensitivity, specificity, precision, F1, detectionrate))
#rocplot
jpeg("Plotleg.jpeg", units="in", width=5.2, height=5.2,res = 2000)
plot(roc1, col = "darkgreen", xlab = "", ylab = "")
par(new = T)
plot(roc, col = "darkblue", xlab = "False Positive Rate (1 - Specificity)", ylab = "True Positive Rate (Sensitivity)")
abline(a = 0, b = 1, lwd = 1, lty = 2)
title(main = "Leg Pain")
legend(0.6,0.13, legend = c("Deep Learning", "Logistic Regression"), col = c("darkgreen", "darkblue"), lty = 1, cex = 0.7)
dev.off()
#densityplot
rocdata$Outcome <- factor(rocdata$target, levels=c(0, 1), labels=c("True non-MCID", "True MCID"))
jpeg("Plotdens.jpeg", units="in", width=5.2, height=5.2,res = 2000)
p <- ggplot(rocdata, aes(x = probability, fill = Outcome)) +
geom_density(alpha=0.2, adjust = 1/0.9) +
xlab("Predicted Probability") +
ylab("Density") +
scale_color_calc(name = "True Outcome at 1 Year",
labels = c("No MCID", "MCID")) + theme_bw() + theme(legend.position = c(0.2, 0.885))
p
dev.off()
shiny::runApp('shiny')
library(rPython)
install.packages("PythonInR")
install.packages("rPython")
install.packages("RPyGeo")
install.packages("rpy2")
install.packages("rpy2
")
install.packages("rpy2")
install.packages("rPython")
update.packages()
library(reticulate)
shiny::runApp('shiny')
install.packages("yaml")
runApp('shiny')
install.packages("rsconnect")
shiny::runApp('shiny')
rsconnect::appDependencies()
rsconnect::appDependencies()
library(shiny)
library(keras)
library(openxlsx)
library(PythonInR)
library(reticulate)
library(yaml)
library(rsconnect)
devtools::install_github(rPython)
rsconnect::deployApp()
devtools::install_github("rPython")
devtools::install_github(cran/rPython)
devtools::install_github("cran/rPython")
rsconnect::deployApp()
devtools::install_github("cran/rPython")
library(shiny)
library(keras)
library(openxlsx)
library(tensorflow)
library(PythonInR)
library(reticulate)
library(yaml)
library(rsconnect)
library(devtools)
library(rPython)
library(RJSONIO)
library(werkzeug)
library(githubinstall)
library(tfdeploy)
tensorflow::install_tensorflow(method = "auto")
tensorflow::install_tensorflow(method = "auto")
library(shiny)
library(keras)
library(openxlsx)
library(tensorflow)
library(PythonInR)
library(reticulate)
library(yaml)
library(rsconnect)
library(devtools)
library(rPython)
library(RJSONIO)
library(werkzeug)
library(githubinstall)
library(tfdeploy)
tensorflow::install_tensorflow(method = "auto")
shiny::runApp('shiny')
runApp('shiny')
runApp('shiny')
runApp('shiny')
runApp('shiny')
runApp('shiny')
install.packages("crayon")
library(crayon)
runApp('shiny')
runApp('shiny')
runApp('shiny')
runApp('shiny')
install.packages("tensorflow")
library(rsconnect)
rsconnect::setAccountInfo(name = "neurospine",
token = "EA404C5F9F63283B33FA3D5024D84E4F",
secret = "Y+uOPLuIjwjyk0a9l/9mAtCJRkKUGYuHxwig4/Sh")
rsconnect::deployApp("D:/Google Drive/Research/Bergman Clinics/ML Deep Learning/ML Predicting outcomes degenerative spine/Ver0 VES DL SpineOutcomes/shiny", appName = "DeepLDH", account = "neurospine")
library(rsconnect)
rsconnect::setAccountInfo(name = "neurospine",
token = "EA404C5F9F63283B33FA3D5024D84E4F",
secret = "Y+uOPLuIjwjyk0a9l/9mAtCJRkKUGYuHxwig4/Sh")
rsconnect::deployApp("D:/Google Drive/Research/Bergman Clinics/ML Deep Learning/ML Predicting outcomes degenerative spine/Ver0 VES DL SpineOutcomes/shiny", appName = "DeepLDH", account = "neurospine")
library(rsconnect)
rsconnect::setAccountInfo(name = "neurospine",
token = "EA404C5F9F63283B33FA3D5024D84E4F",
secret = "Y+uOPLuIjwjyk0a9l/9mAtCJRkKUGYuHxwig4/Sh")
rsconnect::deployApp("D:/Google Drive/Research/Bergman Clinics/ML Deep Learning/ML Predicting outcomes degenerative spine/Ver0 VES DL SpineOutcomes/shiny", appName = "DLDH", account = "neurospine")
shiny::runApp()
runApp()
runApp()
install.packages("packrat")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
